{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast Style Transfer Tensorflow 2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM1bqoGdgCbX"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-rer8IOqPP6"
      },
      "source": [
        "!pip install --upgrade tensorflow==2.1.0\n",
        "!pip install google-api-python-client\n",
        "!pip install apiclient\n",
        "!pip install  --no-deps tensorflow-addons~=0.6\n",
        "!pip install typeguard==2.7.1\n",
        "!pip install -U tensorflow-gpu==2.1.0 grpcio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdvG5n3BClJy"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('GPU device found!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2woEzIb6ch3W"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_QH9XMCvoBm"
      },
      "source": [
        "# This will free up some disk space before we import the dataset, which is ~13 GB.\n",
        "!rm -rf /usr/local/lib/python2.7\n",
        "!rm -rf /swift\n",
        "!rm -rf /tensorflow-1.15.2/python2.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xbs7jSy3ROp"
      },
      "source": [
        "!mkdir /tmp/data\n",
        "!mkdir /tmp/data/train2014"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0lqaqlhC1fD"
      },
      "source": [
        "# Retrieve the dataset. I saved a copy of the zip file to Google Drive \n",
        "# to avoid making requests each time I openned Colab.\n",
        "!wget http://msvocds.blob.core.windows.net/coco2014/train2014.zip -O /tmp/train2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVtgoCt0D6cu"
      },
      "source": [
        "!unzip -q /tmp/train2014.zip -d /tmp/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3dUY0jQb5Ia"
      },
      "source": [
        "!rm /tmp/train2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUB1ZmfpZUjd"
      },
      "source": [
        "!wget -q https://static.wixstatic.com/media/507339_37d143096d6249f3b7253a4b474a1a3a~mv2.jpg -O /tmp/udnie.jpg\n",
        "!wget -q https://static.wixstatic.com/media/507339_e9064ed1c95d4e1eb76d31ff13117d9e~mv2.jpg -O /tmp/mosaic.jpg\n",
        "!wget -q https://static.wixstatic.com/media/507339_f11f19449cd54141a1e95ad4b4419ccf~mv2.jpg -O /tmp/the_scream.jpg\n",
        "!wget -q https://static.wixstatic.com/media/507339_62306b21cef045d599ac272aa410ca3a~mv2.jpg -O /tmp/rain_princess.jpg\n",
        "!wget -q https://static.wixstatic.com/media/507339_b3b8b65faa854e83b6924344594b2807~mv2.jpg -O /tmp/wave.jpg\n",
        "!wget -q https://static.wixstatic.com/media/507339_0d4a0400a92c4db28353d9daab20bad0~mv2.jpg -O /tmp/chicago.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ysJgTHRctY2"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F2-z5lcm8oc"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from skimage import transform\n",
        "\n",
        "def get_image(img_path, img_size=False):\n",
        "  img = load_img(img_path)\n",
        "  img = img_to_array(img, dtype=np.float32)\n",
        "  if img_size != False:\n",
        "    img = resize_img(img, img_size)\n",
        "  return img\n",
        "\n",
        "def resize_img(img, size):\n",
        "  if len(size) == 2:\n",
        "    size += (3,)\n",
        "  return transform.resize(img, size, preserve_range=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J90rLiY2K9z6"
      },
      "source": [
        "from tensorflow.keras.layers import Activation, Add, BatchNormalization, Conv2D, Conv2DTranspose, Layer\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "\n",
        "\n",
        "class ConvLayer(Layer):\n",
        "  def __init__(self, filters, \n",
        "               kernel=(3,3), padding='same', \n",
        "               strides=(1,1), activate=True, name=\"\", \n",
        "               weight_initializer=\"glorot_uniform\"\n",
        "               ):\n",
        "    super(ConvLayer, self).__init__()\n",
        "    self.activate = activate\n",
        "    self.conv = Conv2D(filters, kernel_size=kernel, \n",
        "                       padding=padding, strides=strides, \n",
        "                       name=name, trainable=True,\n",
        "                       use_bias=False, \n",
        "                       kernel_initializer=weight_initializer)\n",
        "    self.inst_norm = InstanceNormalization(axis=3, \n",
        "                                          center=True, \n",
        "                                          scale=True, \n",
        "                                          beta_initializer=\"zeros\", \n",
        "                                          gamma_initializer=\"ones\",\n",
        "                                          trainable=True)\n",
        "    if self.activate:\n",
        "      self.relu_layer = Activation('relu', trainable=False)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.inst_norm(x)\n",
        "    if self.activate:\n",
        "      x = self.relu_layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class ResBlock(Layer):\n",
        "  def __init__(self, filters, kernel=(3,3), padding='same', weight_initializer=\"glorot_uniform\", prefix=\"\"):\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.prefix_name = prefix + \"_\"\n",
        "    self.conv1 = ConvLayer(filters=filters, \n",
        "                           kernel=kernel, \n",
        "                           padding=padding, \n",
        "                           weight_initializer=weight_initializer,\n",
        "                           name=self.prefix_name + \"conv_1\")\n",
        "    self.conv2 = ConvLayer(filters=filters, \n",
        "                           kernel=kernel, \n",
        "                           padding=padding, \n",
        "                           activate=False, \n",
        "                           weight_initializer=weight_initializer,\n",
        "                           name=self.prefix_name + \"conv_2\")\n",
        "    self.add = Add(name=self.prefix_name + \"add\")\n",
        "\n",
        "  def call(self, x):\n",
        "    tmp = self.conv1(x)\n",
        "    c = self.conv2(tmp)\n",
        "    return self.add([x, c])\n",
        "\n",
        "\n",
        "class ConvTLayer(Layer):\n",
        "  def __init__(self, filters, kernel=(3,3), padding='same', strides=(1,1), activate=True, name=\"\",\n",
        "               weight_initializer=\"glorot_uniform\" \n",
        "               ):\n",
        "    super(ConvTLayer, self).__init__()\n",
        "    self.activate = activate\n",
        "    self.conv_t = Conv2DTranspose(filters, kernel_size=kernel, padding=padding, \n",
        "                                  strides=strides, name=name, \n",
        "                                  use_bias=False,\n",
        "                                  kernel_initializer=weight_initializer)\n",
        "    self.inst_norm = InstanceNormalization(axis=3, \n",
        "                                          center=True, \n",
        "                                          scale=True, \n",
        "                                          beta_initializer=\"zeros\", \n",
        "                                          gamma_initializer=\"ones\",\n",
        "                                          trainable=True)\n",
        "    if self.activate:\n",
        "      self.relu_layer = Activation('relu')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv_t(x)\n",
        "    x = self.inst_norm(x)\n",
        "    if self.activate:\n",
        "      x = self.relu_layer(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncmIS-AjqRJ_"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, Add, Layer, Conv2DTranspose, Activation\n",
        "\n",
        "class TransformNet:\n",
        "  def __init__(self):\n",
        "    self.conv1 = ConvLayer(32, (9,9), strides=(1,1), padding='same', name=\"conv_1\")\n",
        "    self.conv2 = ConvLayer(64, (3,3), strides=(2,2), padding='same', name=\"conv_2\")\n",
        "    self.conv3 = ConvLayer(128, (3,3), strides=(2,2), padding='same', name=\"conv_3\")\n",
        "    self.res1 = ResBlock(128, prefix=\"res_1\")\n",
        "    self.res2 = ResBlock(128, prefix=\"res_2\")\n",
        "    self.res3 = ResBlock(128, prefix=\"res_3\")\n",
        "    self.res4 = ResBlock(128, prefix=\"res_4\")\n",
        "    self.res5 = ResBlock(128, prefix=\"res_5\")\n",
        "    self.convt1 = ConvTLayer(64, (3,3), strides=(2,2), padding='same', name=\"conv_t_1\")\n",
        "    self.convt2 = ConvTLayer(32, (3,3), strides=(2,2), padding='same', name=\"conv_t_2\")\n",
        "    self.conv4 = ConvLayer(3, (9,9), strides=(1,1), padding='same', activate=False, name=\"conv_4\")\n",
        "    self.tanh = Activation('tanh')\n",
        "    self.model = self._get_model()\n",
        "\n",
        "  def _get_model(self):\n",
        "    inputs = tf.keras.Input(shape=(None,None,3))\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.res1(x)\n",
        "    x = self.res2(x)\n",
        "    x = self.res3(x)\n",
        "    x = self.res4(x)\n",
        "    x = self.res5(x)\n",
        "    x = self.convt1(x)\n",
        "    x = self.convt2(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.tanh(x)\n",
        "    x = (x + 1) * (255. / 2)\n",
        "    return tf.keras.Model(inputs, x, name=\"transformnet\")\n",
        "\n",
        "  def get_variables(self):\n",
        "    return self.model.trainable_variables\n",
        "\n",
        "  def preprocess(self, img):\n",
        "    return img / 255.0\n",
        "\n",
        "  def postprocess(self, img):\n",
        "    return tf.clip_by_value(img, 0.0, 255.0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POvVhUA425-V"
      },
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras import Model\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "VGG_Output = namedtuple('VGG_Output', 'content_output style_output')\n",
        "\n",
        "class VGGModel:\n",
        "  def __init__(self,\n",
        "               content_layers=[\"conv4_2\"],\n",
        "               style_layers=[\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]\n",
        "               ):\n",
        "    self.vgg = VGG19(include_top=False, weights='imagenet')\n",
        "    self.layers = {\n",
        "      \"input\"  : 0,\n",
        "      \"conv1_1\": 1,\n",
        "      \"conv1_2\": 2,\n",
        "      \"pool1\"  : 3,\n",
        "      \"conv2_1\": 4,\n",
        "      \"conv2_2\": 5,\n",
        "      \"pool2\"  : 6,\n",
        "      \"conv3_1\": 7,\n",
        "      \"conv3_2\": 8,\n",
        "      \"conv3_3\": 9,\n",
        "      \"conv3_4\": 10,\n",
        "      \"pool3\"  : 11,\n",
        "      \"conv4_1\": 12,\n",
        "      \"conv4_2\": 13,\n",
        "      \"conv4_3\": 14,\n",
        "      \"conv4_4\": 15,\n",
        "      \"pool4\"  : 16,\n",
        "      \"conv5_1\": 17,\n",
        "      \"conv5_2\": 18,\n",
        "      \"conv5_3\": 19,\n",
        "      \"conv5_4\": 20,\n",
        "      \"pool5\"  : 21,\n",
        "      \"flatten\": 22,\n",
        "      \"fc1\"    : 23,\n",
        "      \"fc2\"    : 24,\n",
        "      \"predictions\": 25,\n",
        "    }\n",
        "    self.content_layers = content_layers\n",
        "    self.style_layers = style_layers\n",
        "    self.total_output_layers = self.content_layers + self.style_layers\n",
        "    self.partition_idx = len(self.content_layers)\n",
        "    self.model = Model(self.vgg.inputs, self._get_outputs(), trainable=False)\n",
        "\n",
        "  def forward(self, X):\n",
        "    outputs = self.model(X)\n",
        "    return VGG_Output(outputs[:self.partition_idx], outputs[self.partition_idx:])\n",
        "\n",
        "  def _get_outputs(self):\n",
        "    return [self.vgg.layers[self.layers[layer]].output for layer in self.total_output_layers]\n",
        "\n",
        "  def preprocess(self, images):\n",
        "    images = tf.keras.applications.vgg19.preprocess_input(images)\n",
        "    images = tf.cast(images, tf.float32)\n",
        "    return images\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3MzphosdGlA"
      },
      "source": [
        "## Training\n",
        "The main training loop. \n",
        "\n",
        "Note: The log and save protocols were largely used for development. In other implementations, I added to them to copy SavedModels and log files to Google Drive in order to check in on the training model. This is why I used SavedModels over checkpoints. Feel free to copy the notebook and do likewise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zKYWjfEGWMA"
      },
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img \n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from collections import namedtuple\n",
        "from glob import glob\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "Loss = namedtuple('Loss', 'total_loss style_loss content_loss tv_loss')\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, \n",
        "               style_path, \n",
        "               content_file_path, \n",
        "               epochs=2, \n",
        "               batch_size=8,\n",
        "               content_weight=1e0,\n",
        "               style_weight=4e1,\n",
        "               tv_weight=2e2,\n",
        "               learning_rate=1e3,\n",
        "               log_period=100,\n",
        "               save_period=1000,\n",
        "               content_layers=[\"conv4_2\"],\n",
        "               style_layers=[\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"],\n",
        "               content_layer_weights=[1],\n",
        "               style_layer_weights=[0.2, 0.2, 0.2, 0.2, 0.2]):\n",
        "    self.style_path = style_path\n",
        "    self.style_name = style_path.split(\"/\")[-1].split(\".\")[0]\n",
        "    self.content_file_path = content_file_path\n",
        "    assert(len(content_layers) == len(content_layer_weights))\n",
        "    self.content_layers = content_layers\n",
        "    self.content_layer_weights = content_layer_weights\n",
        "    assert(len(style_layers) == len(style_layer_weights))\n",
        "    self.style_layers = style_layers\n",
        "    self.style_layer_weights = style_layer_weights\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.log_period = log_period\n",
        "    self.save_period = save_period\n",
        "    self.saved_model_path = \"/tmp/saved_models\"\n",
        "\n",
        "    self.style_weight = style_weight\n",
        "    self.content_weight = content_weight\n",
        "    self.tv_weight = tv_weight\n",
        "\n",
        "    self.transform = TransformNet()\n",
        "    self.vgg = VGGModel(content_layers, style_layers)\n",
        "    self.learing_rate = learning_rate\n",
        "    self.train_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learing_rate)\n",
        "\n",
        "  def run(self):\n",
        "    self.S_outputs = self._get_S_outputs()\n",
        "    self.S_style_grams = [self._gram_matrix(tf.convert_to_tensor(m, tf.float32)) for m in self.S_outputs[self.vgg.partition_idx:]]\n",
        "\n",
        "    content_images = glob(os.path.join(self.content_file_path, \"*.jpg\"))\n",
        "    num_images = len(content_images) - (len(content_images) % self.batch_size)\n",
        "    print(\"Training on %d images\" % num_images)\n",
        "\n",
        "    self.iteration = 0\n",
        "\n",
        "    for e in range(self.epochs):\n",
        "      for e_i, batch in enumerate([content_images[i:i+self.batch_size] for i in range(0, num_images, self.batch_size)]):\n",
        "\n",
        "        content_imgs = [get_image(img_path, (256,256,3)) for img_path in batch]\n",
        "        content_imgs = np.array(content_imgs)\n",
        "        content_tensors = tf.convert_to_tensor(content_imgs)      \n",
        "\n",
        "        loss = self._train_step(content_tensors)\n",
        "\n",
        "        if (self.iteration % self.log_period == 0):\n",
        "          self._log_protocol(loss)     \n",
        "        if (self.iteration % self.save_period == 0):\n",
        "          self._save_protocol()\n",
        "\n",
        "        self.iteration += 1\n",
        "\n",
        "      self._log_protocol(loss)\n",
        "      self._save_protocol()\n",
        "      print(\"Epoch complete.\")\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "  def _get_S_outputs(self):\n",
        "    img = tf.convert_to_tensor(get_image(self.style_path), tf.float32)\n",
        "    img = tf.expand_dims(img, 0)\n",
        "    img = self.vgg.preprocess(img)\n",
        "    return self.vgg.model(img)\n",
        "\n",
        "  @tf.function\n",
        "  def _train_step(self, content_tensors):\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "      tape.watch(self.transform.get_variables())\n",
        "      C = self.transform.preprocess(content_tensors)\n",
        "      X = self.transform.model(C)\n",
        "      X = self.transform.postprocess(X)\n",
        "\n",
        "      X_vgg = self.vgg.preprocess(X)\n",
        "      Y_hat = self.vgg.forward(X_vgg) \n",
        "      Y_hat_content = Y_hat.content_output\n",
        "      Y_hat_style = Y_hat.style_output\n",
        "\n",
        "      C_vgg = self.vgg.preprocess(content_tensors)\n",
        "      Y = self.vgg.forward(C_vgg)\n",
        "      Y_content = Y.content_output\n",
        "\n",
        "      L = self._get_loss(Y_hat_content, Y_hat_style, Y_content, X)\n",
        "    grads = tape.gradient(L.total_loss, self.transform.get_variables())\n",
        "    self.train_optimizer.apply_gradients(zip(grads, self.transform.get_variables()))\n",
        "    return L\n",
        "\n",
        "  def _get_loss(self, transformed_content, transformed_style, content, transformed_img):\n",
        "\n",
        "    content_loss = self._get_content_loss(transformed_outputs=transformed_content, content_outputs=content)\n",
        "    style_loss = self._get_style_loss(transformed_style)\n",
        "    tv_loss = self._get_total_variation_loss(transformed_img)\n",
        "\n",
        "    L_style = style_loss * self.style_weight\n",
        "    L_content = content_loss * self.content_weight\n",
        "    L_tv = tv_loss * self.tv_weight\n",
        "\n",
        "    total_loss = L_style + L_content + L_tv\n",
        "    \n",
        "    return Loss(total_loss=total_loss, \n",
        "                style_loss=L_style, \n",
        "                content_loss=L_content,\n",
        "                tv_loss=L_tv)\n",
        "    \n",
        "  def _get_content_loss(self, transformed_outputs, content_outputs):\n",
        "    content_loss = 0\n",
        "    assert(len(transformed_outputs) == len(content_outputs))\n",
        "    for i, output in enumerate(transformed_outputs):\n",
        "      weight = self.content_layer_weights[i]\n",
        "      B, H, W, CH = output.get_shape()\n",
        "      HW = H * W\n",
        "      loss_i = weight * 2 * tf.nn.l2_loss(output-content_outputs[i]) / (B*HW*CH)\n",
        "      content_loss += loss_i\n",
        "    return content_loss\n",
        "\n",
        "  def _get_style_loss(self, transformed_outputs):\n",
        "    style_loss = 0\n",
        "    assert(len(transformed_outputs) == len(self.S_style_grams))\n",
        "    for i, output in enumerate(transformed_outputs):\n",
        "      weight = self.style_layer_weights[i]\n",
        "      B, H, W, CH = output.get_shape()\n",
        "      G = self._gram_matrix(output)\n",
        "      A = self.S_style_grams[i]\n",
        "      style_loss += weight * 2 * tf.nn.l2_loss(G - A) / (B * (CH ** 2))\n",
        "    return style_loss\n",
        "    \n",
        "  def _gram_matrix(self, input_tensor, shape=None):\n",
        "      result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "      input_shape = input_tensor.get_shape()\n",
        "      num_locations = input_shape[1] * input_shape[2] * input_shape[3]\n",
        "      num_locations = tf.cast(num_locations, tf.float32)\n",
        "      return result / num_locations\n",
        "\n",
        "  def _get_total_variation_loss(self, img):\n",
        "    B, W, H, CH = img.get_shape()\n",
        "    return tf.reduce_sum(tf.image.total_variation(img)) / (W*H)\n",
        "\n",
        "  def _log_protocol(self, L):\n",
        "    tf.print(\"iteration: %d, total_loss: %f, style_loss: %f, content_loss: %f, tv_loss: %f\" \\\n",
        "                  % (self.iteration, L.total_loss, L.style_loss, L.content_loss, L.tv_loss))\n",
        "\n",
        "  def _save_protocol(self):\n",
        "    tf.keras.models.save_model(model=self.transform.model, filepath=self.saved_model_path)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqUU8XLHzbuc"
      },
      "source": [
        "trainer = Trainer(\n",
        "  style_path=\"/tmp/udnie.jpg\", \n",
        "  content_file_path=\"/tmp/data/train2014\", \n",
        "  epochs=2, \n",
        "  batch_size=8,\n",
        "  content_weight=1e0,\n",
        "  style_weight=4e1,\n",
        "  tv_weight=2e2,\n",
        "  learning_rate=1e-3,\n",
        "  log_period=100,\n",
        "  save_period=1000,\n",
        "  content_layers=[\"conv4_2\"],\n",
        "  style_layers=[\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"],\n",
        "  content_layer_weights=[1],\n",
        "  style_layer_weights=[0.2, 0.2, 0.2, 0.2, 0.2]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl3AwMGKlNMR"
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfePL8E-eua2"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPOUhKmfVO3d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbJ7B1u3UlJo"
      },
      "source": [
        "def post_process(img):\n",
        "  img = tf.clip_by_value(img, 0, 255)\n",
        "  img = img.numpy()\n",
        "  img = tf.squeeze(img)\n",
        "  img = img.numpy()\n",
        "  img = img.astype(int)\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SOrEakPT7XK"
      },
      "source": [
        "img = get_image(\"/tmp/chicago.jpg\")\n",
        "img_tensor = tf.convert_to_tensor(img)\n",
        "img_tensor = tf.expand_dims(img_tensor, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRpO8JEiUFSw"
      },
      "source": [
        "res = trainer.transform.model(img_tensor)\n",
        "res = post_process(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fgTFdkHU7K6"
      },
      "source": [
        "plt.figure(1, (10,10))\n",
        "plt.imshow(res)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}